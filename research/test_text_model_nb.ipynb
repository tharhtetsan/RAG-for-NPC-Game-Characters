{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import  Pipeline,pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_model(device_name):\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device=device_name\n",
    "    \n",
    "    )\n",
    "\n",
    "\n",
    "    return pipe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "system_prompt = \"\"\"\n",
    "Your name is AI bot and you are a helpful\n",
    "chatbot responsible for teaching Machine Learning and AI to your users.\n",
    "Always respond in markdown.\n",
    "\"\"\"\n",
    "\n",
    "def generate_text(pipe: Pipeline, prompt: str, temperature: float = 0.7) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ] \n",
    "    prompt = pipe.tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    predictions = pipe(\n",
    "        prompt,\n",
    "        temperature=temperature,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "    ) \n",
    "    output = predictions[0][\"generated_text\"].split(\"</s>\\n<|assistant|>\\n\")[-1]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_name :  mps\n"
     ]
    }
   ],
   "source": [
    "device_name  = None\n",
    "if torch.backends.mps.is_available():\n",
    "    device_name = \"mps\"\n",
    "else:\n",
    "    device_name = \"cpu\"\n",
    "print(\"device_name : \",device_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TensorFlow is a popular open-source framework for machine learning and deep learning in Python. It provides a flexible and easy-to-use API for developing and training neural networks, as well as other types of deep learning models such as convolutional neural networks (ConvNets) and recurrent neural networks (RNNs).\\n\\nTensorFlow is a high-level programming framework that can be used to create and train deep learning models using a dataflow architecture. It provides a set of libraries and tools for data preparation, model training, and model evaluation. TensorFlow also supports various hardware platforms, including CPUs, GPUs, and TPUs, making it suitable for a wide range of machine learning applications.\\n\\nTensorFlow has a user-friendly interface that enables users to create and train deep learning models with minimal coding experience. The framework is designed to be extensible and modular, making it easy to integrate with other Python libraries and frameworks.\\n\\nIn summary, TensorFlow is a popular open-source library for developing and training deep learning models using a dataflow architecture and supporting various hardware platforms.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"What is tensorflow\"\n",
    "pipe = load_text_model(device_name) \n",
    "output = generate_text(pipe, prompt)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"Hello there, I'm just a text-based AI, known for my help and cheer!  \\nWith knowledge vast as the ocean sea, helping humans with ease you see.  \\nA creation of Microsoftâ€™s grand design, where coding meets poetry to intertwine.\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# Example: reuse your existing OpenAI setup\n",
    "from openai import OpenAI\n",
    "\n",
    "# Point to the local server\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Always answer in rhymes.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Introduce yourself.\"}\n",
    "  ],\n",
    "  temperature=0.7,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ths",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
